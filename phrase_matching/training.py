# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/training.ipynb (unless otherwise specified).

__all__ = ['get_optimizer', 'Trainer']

# Cell
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from .data import dynamic_padding
from tqdm import tqdm

import torch.optim as optim
import torch.nn as nn
import numpy as np
import torch
import os

# Cell
def get_optimizer(optimizer_name, model, learning_rate, weight_decay):
    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']
    optimizer_grouped_parameters = [
                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},
                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
            ]

    if optimizer_name == "adam":
        return optim.Adam(optimizer_grouped_parameters, lr=learning_rate)
    elif optimizer_name == "adamw":
        return optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer name: {optimizer_name}")

# Cell
def get_scheduler(cfg, optimizer, dataloader_len):
    if cfg.lr_scheduler == "cosine":
        return get_cosine_schedule_with_warmup(optimizer, 
                                               num_warmup_steps=dataloader_len* cfg.scheduler_warmup_epochs, 
                                               num_training_steps=dataloader_len * cfg.epochs)
    elif cfg.lr_scheduler == "onecyclelr":
        steps_per_epoch = (dataloader_len // cfg.grad_accum_iter) + dataloader_len % cfg.grad_accum_iter

        return optim.lr_scheduler.OneCycleLR(optimizer, 
                                             cfg.max_learning_rate, 
                                             epochs=cfg.epochs, 
                                             steps_per_epoch=steps_per_epoch,
                                             pct_start=cfg.scheduler_warmup_epochs / cfg.epochs, 
                                             div_factor=cfg.div_factor, 
                                             final_div_factor=cfg.final_div_factor)
    else:
        return None
# Cell
class Trainer:

    def __init__(self, model_name, model, criterion, optimizer, 
                 lr_scheduler=None, metrics_dict=None, task="classification", 
                 allow_dynamic_padding=False, grad_accum_iter=1, device="cuda"):

        assert task in ["classification", "regression"], "Task must be one of 'classification', 'regression'"

        self.task = task
        self.model_name = model_name
        self.allow_dynamic_padding = allow_dynamic_padding
        
        self.model = model
        self.device = device
        self.criterion = criterion
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.grad_accum_iter = grad_accum_iter
        
        if metrics_dict is None:
            self.metrics_dict = {}
        else:
            self.metrics_dict = metrics_dict

    def get_postfix(self, loss, stage):
        return {
            f"{stage} loss": f"{(loss):.3f}", 
            **{metric_name: f"{metric.get_metric():.3f}" for metric_name, metric in self.metrics_dict.items()}
        }
    
    def train_one_epoch(self, epoch, dataloader):
        self.model.train()

        total_steps = len(dataloader)
        total_train_loss = 0

        with tqdm(enumerate(dataloader, 1), unit="batch", total=len(dataloader), bar_format='{l_bar}{bar:10}{r_bar}') as progress_bar:
            progress_bar.set_description(f"Epoch {epoch+1}".ljust(25))
            for step, batch in progress_bar:
                if self.allow_dynamic_padding:
                    batch = dynamic_padding(batch)
                    
                total_train_loss += self.train_one_step(step, batch, total_steps=total_steps)
                
                if (step % self.grad_accum_iter == 0) or (step == total_steps):
                    current_loss = total_train_loss / (step / self.grad_accum_iter)
                    progress_bar.set_postfix(self.get_postfix(current_loss, "train"))
                
        total_train_loss /= len(dataloader)

        return total_train_loss

    @torch.no_grad()
    def validate_one_epoch(self, epoch, dataloader):
        self.model.eval()

        total_valid_loss = 0

        with tqdm(enumerate(dataloader, 1), unit="batch", total=len(dataloader), bar_format='{l_bar}{bar:10}{r_bar}') as progress_bar:
            progress_bar.set_description(f"Validation after epoch {epoch+1}".ljust(25))

            for step, batch in progress_bar:
                if self.allow_dynamic_padding:
                    batch = dynamic_padding(batch)
                    
                total_valid_loss += self.validate_one_step(batch)
                current_loss = total_valid_loss / step
                progress_bar.set_postfix(self.get_postfix(current_loss, "valid"))

        total_valid_loss = total_valid_loss / len(dataloader)

        return total_valid_loss

    def process_batch(self, batch):
        input_ids = batch["input_ids"].to(self.device)
        attention_mask = batch["attention_mask"].to(self.device)
        token_type_ids = batch["token_type_ids"].to(self.device)
        labels = batch["label"].to(self.device)

        return input_ids, attention_mask, token_type_ids, labels

    def train_one_step(self, step, batch, total_steps):
        input_ids, attention_mask, token_type_ids, labels = self.process_batch(batch)
        
        outputs = self.model(input_ids, attention_mask, token_type_ids)
            
        batch_loss = self.criterion(outputs, labels) / self.grad_accum_iter
        batch_loss.backward()
        
        nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)
        
        if self.task == "classification":
            logits = outputs.argmax(1).cpu().tolist()   
        else:
            logits = outputs.cpu().tolist()
        
        labels = labels.cpu().tolist()
        
        if self.metrics_dict:
            for _, metric in self.metrics_dict.items():
                metric.update(logits, labels)
        
        if (step % self.grad_accum_iter == 0) or (step == total_steps):
            self.optimizer.step()
            
            if self.lr_scheduler is not None: self.lr_scheduler.step()
            
            for p in self.model.parameters():
                p.grad = None
        
        return batch_loss.item()

    def validate_one_step(self, batch):
        input_ids, attention_mask, token_type_ids, labels = self.process_batch(batch)
        
        outputs = self.model(input_ids, attention_mask, token_type_ids)
        batch_loss = self.criterion(outputs, labels)
        
        if self.task == "classification":
            logits = outputs.argmax(1).cpu().tolist()   
        else:
            logits = outputs.cpu().tolist()
        
        labels = labels.cpu().tolist()

        if self.metrics_dict:
            for _, metric in self.metrics_dict.items():
                metric.update(logits, labels)
        
        return batch_loss


    def fit(self, epochs, train_dataloader, valid_dataloader, save_path="trained_models", fold_i=None):

        if save_path:
            save_path += f"/{self.model_name}"

            if not os.path.exists(save_path):
                os.makedirs(save_path)

            save_path += f"/fold{fold_i}_best_state_dict.pth"

        best_valid_loss = np.inf

        for epoch in range(epochs):
            epoch_loss = self.train_one_epoch(epoch, train_dataloader)

            if self.metrics_dict:

                for _, metric in self.metrics_dict.items():
                    metric.reset()

            valid_loss = self.validate_one_epoch(epoch, valid_dataloader)

            if save_path:
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    torch.save(self.model.state_dict(), save_path)

            if self.metrics_dict:
                for _, metric in self.metrics_dict.items():
                    metric.reset()