# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/data.ipynb (unless otherwise specified).

__all__ = ['PhraseDataset']

# Cell
from torch.utils.data import Dataset

import numpy as np
import torch

# Cell
def dynamic_padding(batch, pad_token_idx):
    padding_idx = np.argmax(batch["input_ids"].numpy() == pad_token_idx, axis=1).max()
    
    if padding_idx == 0:
        return batch
    batch["input_ids"] = batch["input_ids"][:, :padding_idx]
    batch["attention_mask"] = batch["attention_mask"][:, :padding_idx]
    batch["token_type_ids"] = batch["token_type_ids"][:, :padding_idx]
    
    return batch
    
# Cell
class PhraseDataset(Dataset):
    def __init__(self, dataset_df, tokenizer, max_len=128, lowercase=True, mode="train_val", task="classification"):
        super().__init__()

        assert task in ("classification", "regression"), "Wrong mode. Choose 'classification' or 'regression'"
        assert mode in ("train_val", "inference"), "Wrong mode. Choose 'inference' or 'train_val'"

        self._task = task
        self._mode = mode

        if lowercase:
            dataset_df.loc[:, dataset_df.dtypes == object] = dataset_df.loc[:, dataset_df.dtypes == object].apply(lambda col: col.str.lower())

        self._anchors = dataset_df["anchor"].values
        self._targets = dataset_df["target"].values
        self._contexts = dataset_df["context"].values

        if mode == "train_val":
            self._labels = dataset_df["score"].values
            self._label_dtype = torch.float if self._task == "regression" else torch.long

        self.tokenizer = tokenizer

        self._max_len = max_len
        self._sep_token = f" {tokenizer.sep_token} "

    def __len__(self):
        return len(self._anchors)

    def __getitem__(self, idx):
        anchor = self._anchors[idx]
        target = self._targets[idx]
        context = self._contexts[idx]

        if self._mode == "train_val":
            label = self._labels[idx]

        input_text = self._sep_token.join([anchor, target, context])

        tokenizer_output = self.tokenizer(input_text,
                                          add_special_tokens=True,
                                          truncation=True,
                                          padding="max_length",
                                          max_length=self._max_len,
                                          return_token_type_ids=True,
                                          return_attention_mask=True,
                                          return_tensors="pt")


        if self._mode == "train_val":
            return {
                "input_ids": tokenizer_output["input_ids"].squeeze(),
                "attention_mask": tokenizer_output["attention_mask"].squeeze(),
                "token_type_ids": tokenizer_output["token_type_ids"].squeeze(),
                "label": torch.tensor(label, dtype=self._label_dtype)
            }

        return {
            "input_ids": tokenizer_output["input_ids"].squeeze(),
            "attention_mask": tokenizer_output["attention_mask"].squeeze(),
            "token_type_ids": tokenizer_output["token_type_ids"].squeeze(),
        }
