# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/models.ipynb (unless otherwise specified).

__all__ = ['ContextPooler', 'Backbone', 'PhraseModel']

# Cell
from transformers import AutoModel

import torch.nn.functional as F
import torch.nn as nn
import torch

# Cell
class ContextPooler(nn.Module):
    def __init__(self, hidden_size, dropout):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states):

        context_token = hidden_states[:, 0]
        context_token = self.dropout(context_token)
        pooled_output = self.dense(context_token)
        pooled_output = F.gelu(pooled_output)

        return pooled_output

# Cell
class Backbone(nn.Module):
    def __init__(self, model_name, pool_backbone_output=True, backbone_pooler_dropout=0.15):
        super().__init__()

        self.model_name = model_name
        self.pool_backbone_output = pool_backbone_output

        self.net = AutoModel.from_pretrained(model_name, return_dict=False)
        self.embedding_dim = len(list(self.net.parameters())[-1])

        if self.pool_backbone_output:
            self.pooler = ContextPooler(hidden_size=self.embedding_dim, dropout=backbone_pooler_dropout)

    def forward(self, input_ids, attention_mask, token_type_ids):

        if "deberta" in self.model_name:
            output = self.net(input_ids, attention_mask, token_type_ids)[0]

            if self.pool_backbone_output:
                pooled_output = self.pooler(output)

                return pooled_output

            return output

        else:
            pooled_output, output = self.net(input_ids, attention_mask, token_type_ids)

            if self.pool_backbone_output:
                return pooled_output

            return output


# Cell
class PhraseModel(nn.Module):
    def __init__(self, model_name, embedding_dim=768, pool_backbone_output=True, backbone_pooler_dropout=0.15, task="classification"):

        assert task in ["classification", "regression"], "Wrong task. Choose 'classification' or 'regression'"

        super().__init__()

        self.task = task
        self.num_classes = 1 if self.task == "regression" else 5

        self.model_name = model_name
        self.embedding_dim = embedding_dim

        self.backbone = Backbone(self.model_name, pool_backbone_output=pool_backbone_output, backbone_pooler_dropout=backbone_pooler_dropout)

        self.neck = nn.Sequential(
            nn.LayerNorm(self.backbone.embedding_dim),
        )

        self.head = nn.Sequential(
            nn.Linear(self.backbone.embedding_dim, self.num_classes),
        )

        self.neck.apply(self._init_weights)
        self.head.apply(self._init_weights)

        if hasattr(self.backbone, "pooler"):
            self.backbone.pooler.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=0.01)

            if module.bias is not None:
                module.bias.data.zero_()

        elif isinstance(module, nn.LayerNorm) or isinstance(module, nn.BatchNorm1d):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, token_type_ids):

        x = self.backbone(input_ids, attention_mask, token_type_ids)
        x = self.neck(x)
        x = self.head(x)

        if self.task == "classification":
            return x

        return x.squeeze()